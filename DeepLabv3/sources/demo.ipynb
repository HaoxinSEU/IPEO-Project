{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Forest Mapping by DeepLabV3\n",
    "\n",
    "Do inference and display the semantic segmentation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1.__ Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# local import\n",
    "import custom_model\n",
    "from iou import iou\n",
    "from accuracy import count_for_user_accuracy, count_for_producer_accuracy, count_for_overall_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2.__ Find the hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3.__ Define file paths needed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the weights file\n",
    "weights_dir = 'weights/weights.pt'\n",
    "# path to the test images\n",
    "img_path = \"./image.tif\"\n",
    "# path to the test labels\n",
    "label_path = \"./target.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4.__ Initialize the model with trained weights, and set the model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our trained model\n",
    "model = custom_model.initialize_model(3, keep_feature_extract=True)\n",
    "state_dict = torch.load(weights_dir, map_location=device)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# set the model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5.__ Input one image in the test set, do the transform required by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# define the transforms\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(512, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# apply transforms\n",
    "image = image_transforms(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 6.__ Do the inference, generate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the inference\n",
    "outputs = model(image)[\"out\"]\n",
    "\n",
    "# select the prediction only in the first two classes\n",
    "outputs = outputs[:, :2, :, :]\n",
    "# get the prediction\n",
    "_, preds = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 7.__ Resize the prediction back to the original size, display the image, prediction and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the label\n",
    "label = Image.open(label_path)\n",
    "\n",
    "# resize the prediction to the label size\n",
    "preds = transforms.Resize(size=(label.size[1], label.size[0]), interpolation=transforms.InterpolationMode.NEAREST)(preds)\n",
    "\n",
    "# change color of the prediction and label\n",
    "color_pair_dict = {0: (255, 255, 255), 1: (0, 255, 0), 255: (0, 0, 0)}\n",
    "\n",
    "# convert the prediction and label to numpy array\n",
    "preds = preds.cpu().numpy()\n",
    "preds = np.squeeze(preds)\n",
    "label = np.array(label)\n",
    "label = np.squeeze(label)\n",
    "\n",
    "# convert the prediction and label to RGB image\n",
    "preds = np.vectorize(color_pair_dict.get)(preds).astype(np.uint8)\n",
    "label = np.vectorize(color_pair_dict.get)(label).astype(np.uint8)\n",
    "\n",
    "# display the original image, prediction and label\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
